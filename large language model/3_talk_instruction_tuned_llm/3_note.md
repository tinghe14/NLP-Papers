[event github repo](https://github.com/FourthBrain/Building-with-Instruction-Tuned-LLMs-A-Step-by-Step-Guide)

Outline
- LLM context: supervised fine-tuning, aka Instruction-Tuning
- D1: Instruction-Tuning: OpenLLaMA and Dolly15k with QLoRA
- D2: Fine-tuning Input/Output Schema: BLOOMZ with PEFT-LoRA
- Conclusions

Instruction-Tunning vs 'Fine-Tuning':
- Instruction-Tuning:
  - Following instructions
  - new benchmarks: bias, toxicity, etc... 
- fine-tuning: for specific task
- instruct-tuning is a subset of fine-tuning, which focused on alignment with humans, concerned with following 'instructions' like human

D1: Instruction-Tuning:
[Dolly15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k):
- Dataset Structure:
  - instruction
  - categpory
  - context 
  - reponse? 
- Task:
  - one of tasks: information retrieval 
[OpenLLaMA](https://huggingface.co/openlm-research/open_llama_7b_700bt_preview):
- replicate of LLaMA can commerical use
[QLoRA](https://github.com/artidoro/qlora):
- the big idea: downstream tasks have intrinsically low dimensions
- leverage:
  - quantization using [bitsandbytes](https://github.com/TimDettmers/bitsandbytes)
  - huggingface's PEFT and transformers libraries
 [colab](https://colab.research.google.com/drive/1SRclU2pcgzCkVXpmhKppVbGW4UcCs5xT?usp=sharing):
 - can be run in google colab pro+, even with more hyperparmater
 - note of colabs:
 - massively reduce the model architecutre and parameters
  - qlora_config:
    - task_type: pay attention to your task 
  - bnb_config:
    - the models' weight are frozen and store in 4 bits, when we need taht unquantizatize
  - [Parameter Efficient Fine-Tuning (PEFT)](https://github.com/huggingface/peft)[SFTTtrainer](https://huggingface.co/docs/trl/main/en/sft_trainer):
    - optim: recommend to read paper 
- important notes:
  - training: 
    - supervised fine-tuning
    - 15,000 data points, 5,000 training steps
    - cost: 75 google colab compute units 
    - 4-bit quantizagtion (QLoRA)
  - rule of thumb of instruct-tuning:
    - pick instruction-tuned models off the shelf when building GenAI apps!
 
D2: Fine-tuning Input-Output Schema:
- D1 is supervised tuning, here is unsupervised tuning
- BLOOMZ is fine-tuned(instruction-tuned) on [BLOOM'SxP3 Dataset](https://huggingface.co/bigscience/bloomz-3b)
- [colab](https://colab.research.google.com/drive/1ARmlaZZaKyAg6HTi57psFLPeh0hDRcPX?usp=sharing)
- LoRA:
  - idea: fine-tune task is much better preformance than original model
  - rank: dimension of the model you want to reduce to
  - config:
    - target_modules:
    - CAUSAL_LM: just to look ahead and look back?? 
  - 17 dataset:
    - generated by GPT4, can't commerial
  - format is important, set up prompt, so it can be reproducible
- important note:
  - training: 
    - unsupervised fine-tuning
    - 17 data points, 100 steps
    - cost: <5 google colab compute units
    - 8-bit quantization (LoRA)
  - general process, file-tuning I/O schema for single task superpowers
    - decide who you're building for and how they'll interact with your app
    - try zero-shot and few-shot prompting first
    - then be data-centric about collecting high-quality examples!

Conclusions:
1. Instruction-tuning is a subset of fine-tuning, focused on following instructions and aligning with humans
2. fine-tuning of input-output schema lowers the dimensionality of the LLM to focus the model on one task
3. rule of thumb: pick up instruction-tuned models when building!
4. this means that you will be fine-tuning on top of instruct-tuned models 

Q&A:
1. what ways are there to prevent hallucinations and ensure that the answers are coming from your embeded data/documents? easy way to let user to check
2. how to make a llm learn from a pdf and answer from the given the pdf? langchain? chain different things together
3. how do you handle confidential data in the LLM while still benefitting from the training of a LLM? some pre- and post-processing steps
4. why is few-short learning called 'learning' since it is not really an extension of the prompted model? it is in-context learning. teach the model to do something you can't do
5. when fine-tuning with custom data, is it better to do it on a (base) model, a supervised model (eg: Dolly) or add the custom instructions to DataBricks?
6. how to get start with llms and what are future scopes? easiest like query chatgpt from python and then move into fine-tuning stuff. follow the tutorial, repeat the process
7. how does the output compare to including 17 examples in the prompt to chatGPT(GPT4)? worse, but low resource to train, might 
8. is it possible for nomral humans to build llms without many computational resources? 
9. when to use 4-bit qlora and when to use 8-bit lora? pros and cons? not well research yet, monitor the performance closely. 
 
