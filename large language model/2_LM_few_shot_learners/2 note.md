- many great statements saying things from my mind
- 'under pre-training plus fine-tuning paradigm, where models are designed to be large to absorb information during pre-training, but are then fine-tuned on very narrow task distributions. Some studies observe that larger models don't neccessarily generalize better out-of-distribution. This is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly specific to the training distribution and doesn't generalize well outside it. Thus, the performance of fine-tuned models on specific benchmarks, even when it is nominally at human-level, may exaggerate actual performance on the underlying task.'
