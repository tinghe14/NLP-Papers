# LM
LM, language model, 语言模型
- 定义：利用自然语言文本构建的，根据输入的文字，输出相应文字的模型。也叫做decoder解码策略
- token:  如果只是用26个英文字母，虽然词表很小（加上各种符号可能就100来个），但粒度太细，每个Token几乎完全没法表示语义；如果用词，这个粒度又有点太大，尤其英文还有不同时态，其实它们意思差不多，只是后面不一样。所以子词就应运而生——它把一个词拆成一定大小的语义单位，每个单位既可以表示一定含义，又能灵活组合。
- greedy search vs beam search: 因为我们考虑的是最大概率，每次都会输出同样的话即为greedy search; 语言模型都会在这个地方做点策略，让模型多看几个可能的单词。而不是就看最高的那一个，这样继续往下找的时候，你会发现到下一步，刚刚最大概率的词，如果加上这一步的词，他的路径可能没有刚刚小一点概率的词的路径大.beam search: 简单的来说就是一步多看几个词，看最终句子（比如生成到句号，感叹号或者其他停止符号）的概率。看得越多，约不容易生成固定的文本

### Ngram
- 最早的LM Ngram：把一句话切成一个个的词，然后统计概率，比如Bi-gram:下一个词是根据前两个词来的
- Ngram的问题 离散：在计算机中，只能用1和0表示某个词。假设词表大小为50000，刚刚Bi-Gram中，「我喜欢」这个Gram就是49999个0和1个1组成的稀疏向量。

### Embedding
- embedding的好处：一种稠密表示方法，简单来说，一个Token是很多个小数（这个很多可以是任意多个，专业称呼为Embedding的维度，看用的模型和设定的参数），一般数字越多，模型越大，表示能力越强。
- 用embedding怎么预测下一个token: 还是算概率，但这次和刚刚有点不一样了，刚刚离散的那就是统计出来有多少次除以总词数就是出现概率。但是稠密向量要稍微换个方式，也就是说，给你一个d维的向量，你最后要输出一个长度为N的向量，N是词表大小，N中每一个值都是一个概率值，表示Token的概率，加起来为1。W即为模型的参数，其实X也可以看作是参数自动学习到。因为我们知道了输入和输出的大小，所以中间其实可以经过任意随意的计算，总之就是各种张量（三维以上数组）运算，只要保证最后的形式不变就行。这中间的各种计算就意味着各种不同的模型。
~~~
X = [0.001, 0.002, 0.0052, ..., 0.0341] # d维，加起来和1没关系，大小是1×d
Y = [0.1, 0.5, ..., 0.005, 0.3] # N个，加起来=1，大小是1×N
W·X = Y  # W自然可以是 d×N 维的矩阵
~~~

### RNN
- 深度学习初期 最著名的语言模型Recurrent Neural Network RNN: RNN模型与其他神经网络不同的地方在于，它的节点之间存在循环链接，这使得它能够记住之前的信息，并将它们应用于当前的输入。这种记忆能力使得 RNN 在处理时间序列数据时特别有用，例如预测未来的时间序列数据、自然语言处理等 
- 
![0 RNN模型参数介绍](https://github.com/tinghe14/NLP-Papers/blob/main/large%20language%20model/0%20RNN%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E4%BB%8B%E7%BB%8D.png)
- 图：第一行是x， 第二行是y，sos： start of sentence。注意，上面的h并不是那个输出的概率，而是hidden state, 如果需要概率，可以将h再做一个张量运算，归一化到整个词表即可
~~~
import torch.nn as nn
rnn = nn.RNN(32, 64)
input = torch.randn(4, 32) # 输入是一个4*32的向量=4个token,维度d=32
h0 = torch.randn(1, 64) # h0就是初始化的输出=output4个里面的第一个
output, hn  = rnn(input, h0)
# output的四个64维的向量就分别表示4个输出
# hn就是最后一个token的输出，也可以看成整个句子的表示
output.shape, hn.shape #(torch.Size([4, 64]), torch.Size([1, 64]))

# 如果要输出词的概率，需要先弄到词表大小，再归一化
wo = torch.randn(64, 1000) # 假设词表大小N=1000
logits = output @ wo  # 4×1000
# probs每一行就是词表大小的概率分布=这个token到词表的每一个单词的概率
probs = nn.Softmax(dim=1)(logits) # 4×1000，每一行概率和为1
~~~
- 图：因为我们知道接下来的token是什么，即y，那这里输出的最大概率的那个token如果正好是这个token,说明预测对了，参数就不用怎么调整；反之，模型就会调整前面的参数（上面的RNN, h0, input的参数和下面的w0）。你可能会疑惑为啥input也是参数，其实上面的input我们偷了懒，本来的参数是1000×32的大向量，4个是那四个Token对应位置的那一行向量。这个1000×32的向量其实就是词向量（每个词一行），开始时随机初始化，然后通过训练调整参数。训练完成后，这些参数就不变了，然后就可以和上面用同样的步骤做预测了，也就是给定一个token预测下一个token。重点需要了解每个token怎么表示，怎么训练和预测出来的。

### Transformer
- trasformer这种架构从更普遍的角度来看，是seq2seq架构：序列到序列模型，也就是输入是一个文本序列，输出是另一个文本序列
- 重要性介绍：接下来出场的是Transformer，一个刚开始在NLP领域，后来横跨到语音和图像领域，并最终统一几乎所有模态的架构。这是Google2017年发的一篇论文，标题叫《Attention Is All You Need》，其最重要的核心就是提出来的Self-Attention机制，中文也叫自注意力。简单来说，就是在语言模型建模过程中，把注意力放在那些重要的Token上。
- encoder-decoder介绍：Transformer是一种Encoder-Decoder架构，简单来说就是先把输入映射到Encoder，这里大家可以把Encoder先想象成上面介绍的RNN，Decoder也可以想象成RNN。这样，左边负责编码，右边则负责解码。这里面不同的是，左边因为我们是知道数据的，所以建模时可以同时利用当前Token的历史Token和未来（前面的）Token；但解码时，因为是一个Token一个Token输出来的，所以只能根据历史Token以及Encoder的Token表示进行建模，而不能利用未来的Token。（这篇博客原文提供了一个很好的gif展示）
- 如何表示整句话：encoder和Decoder可以采用RNN，最终就是Encoder所有Token最终输出一个向量，作为整句话的表示。说到这里，整句话又怎么表示呢？刚刚上面我们也提到过，如果RNN这种结构，可以把最后一个Token的输出作为整个句子的表示。当然了，很符合直觉地，你也可以取每个Token向量的平均值，或第一个和最后一个的平均值，或后面N个的平均值。这些都可以，问题不大，不过一般取平均的情况比较多，效果要好一些。除了平均值，也可以求和、取最大值等，我们就不多深入讨论了。
- gif中decoder的过程：其实它在生成每一个Token时都用到了Encoder每一个Token的信息，以及它已经生成的Token的信息。前面这种关注Encoder中Token的信息的机制就是Attention（注意力机制）。直观点解释，当生成Knowledge时，「知识」两个字会被赋予更多权重，其他也是类似。
- 
![0 transformer的内部结构](https://github.com/tinghe14/NLP-Papers/blob/7cb0b969fcf684dc393117391888c8f15ff25989/large%20language%20model/0%20transformer%E7%9A%84%E5%86%85%E9%83%A8%E7%BB%93%E6%9E%84.png)
- 图：左边是encoder的一个block(一共n个)，右边是decoder的一个block（一共n个）。简单起见，可以假设n=1，那左边的结构就是一个encoder,右边也是一个decoder.可以把它想象成rnn，这样有帮助从宏观角度把握。之后，回到现实，transformer用到的东西和rnn并没有关系，通过上图也可以看出来，它主要用了两个模块：multi-head attention和feed forward
- multi-head attention: 之前gnmt(google neural machine translation)就有attention, 它是decoder中的token和encoder中每一个token的重要性权重。multi-head attention中用到的叫self attention和刚刚说的attention类似，只不过它是自己的每一个token和自己的每一个token的重要性权重。简单来说，就是一句话到底哪里重要。这个机制是很多模型的精髓，无论是chatgpt还是其他非文本的模型，几乎都用到了它，统一了江湖。multi-head的意思：把刚刚这种自己注意自己重复multi次，每个注意到的信息不一样，这样就可以捕获到更多的信息。比如我们前面提过的这句话：「我喜欢在深夜的星空下伴随着月亮轻轻地想你」，有的Head「我」注意到「喜欢」，有的Head「我」注意到「深夜」，有的Head「我」注意到「想你」……这样看起来是不是更加Make Sense。对于Feed Forward，大家可以把它当做「记忆层」，大模型的大部分知识都存在这里面，Multi-Head Attention则根据不同权重的注意提取知识。
- 大部分nlp任务并不是seq2seq的：最常见的主要包括这么几种：句子级别分类、Token级别分类（也叫序列标注）、相似度匹配和生成；而前三种应用最为广泛。这时候Encoder和Decoder就可以拆开用了。左边的Encoder在把句子表示成一个向量时，可以利用上下文信息，也就是说，可以把它看作是双向的；右边的Decoder不能看到未来的Token，一般只利用上文，是单向的。虽然它们都可以用来完成刚刚提到的几个任务，但从效果上来说，Encoder更加适合非生成类任务，Decoder更加适合生成类任务。在NLP领域，一般也会把它们分别叫做NLU（Natural Language Understanding，自然语言理解）任务和NLG（Natural Language Generation，自然语言生成）任务。
- NLU任务: 句子级别分类是给定一个句子，输出一个类别。因为句子可以表示为一个向量，经过张量运算，自然可以映射到每个类的概率分布。这和前面语言模型提到过的搞法没有本质上的区别，只不过语言模型的类别是整个词表大小，而分类的类别是看具体任务的，有二分类、多分类、多标签分类等等。Token级别的分类是给定一个句子，要给其中每个Token输出一个类别。这个和语言模型就更像了，只不过把下一个Token换成是对应的类别，比如命名实体识别就是把句子中的实体（人名、地名、作品等你关注的词，一般是名词）给提取出来。它们的类别一般是类似，如果以人名（PER）举例的话，类似这样：B-PER表示开始、I-PER表示中间。举个例子：「刘亦菲好看」，此时，Token是字，对应的类别为「B-PER、I-PER、I-PER、O、O」，O表示Other。注意，对于分类任务，类别我们一般也叫它标签。相似匹配任务一般是给定两个句子，输出是否相似，其实也可以看作是特殊的分类问题。
- NLG任务：除了生成外，常见的任务还有文本摘要、机器翻译、改写纠错等。这里Seq2Seq的结构就比较常见了，体现了一种先理解再输出的感觉。而纯生成类任务，比如写诗、写歌词、写小说几乎都是Decoder结构。这一类任务稍微麻烦一些的是自动评测，除生成的其他任务还好，一般都会提供参考答案（reference），可以看模型输出的和参考之间重叠程度或相似程度。但纯生成任务就有点麻烦，这个好不好有时候其实很难衡量。不过针对有具体目标的（如任务机器人对话生成），还是可以设计一些是否完成任务、达到目标之类的评测方法。但对于没有具体目标的（比如闲聊），这评测起来就见仁见智了，很多时候还是靠人工过一遍。
- 微调模型时代：Transformer这个架构基于Seq2Seq，可以同时处理NLU和NLG任务，而且这种Self Attention机制的特征提取能力很强。这就使得NLP取得了阶段性的突破，深度学习开始进入了微调模型时代。大概的做法就是拿着一个开源的预训练模型，然后在自己的数据上微调一下，让它能够搞定特定的任务。这个开源的预训练模型往往就是个语言模型，从大量数据语料上，使用我们前面讲的语言模型的训练方法训练而来。NLU领域的第一个工作是Google的BERT，相信不少人即便不是这个行业的也大概听过。BERT就是用了Transformer的Encoder架构，有12个Block（看上面那个图，这每一个Block也可以叫一层），1亿多参数，它不预测下一个Token，而是随机把15%的Token盖住，然后利用其他没盖住的Token来预测盖住的Token。其实和根据上文预测下一个Token是类似的，不同的是可以利用下文信息。NLG领域的第一个工作是OpenAI的GPT，用的是Transformer的Decoder架构，参数和BERT差不多。它们都发表于2018年，然后分别走上了两条不同的路。

### GPT
- Generative pre-trained transformer GPT: 生成式与训练transformer。生成式的意思是类似语言模型那样，token by token生成文本，即 decoder。预训练也提到过了，就是在大量语料上训练的语言模型。gpt模型从1到4，一共经历了5个版本，中间有个chatgpt是3.5版本。
- GPT1: 和bert一样，走的是下游任务微调套路，也就是固定预训练模型不动，然后在不通下游任务上微调一个模型
- 1[0 GPT1基本架构]（https://github.com/tinghe14/NLP-Papers/blob/23c637c3ad02035c650a4510d2fbed94ee8ca0aa/large%20language%20model/0%20GPT1%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84.png）
- gpt1的架构：左边已经介绍过了，用的就是transformer的架构（gpt用的是decoder）。重点在右边：针对不同任务输入，都拼接成文本序列，然后丢给transformer decoder再通过一个linear+softmax输出结构。linear是一种最基础的网络结构，softmax前面介绍过，主要用来把输出映射到概率分布（和为1）。这种拼接输入的方法日后在当时那个大模型时代非常流行的，紧跟其后的BERT也是类似的方式。这样统一的处理方法能够减少不同任务对模型的改动。反正不管什么任务，都想方设法搞成一个序列就行。还有两点有意思，仪式预训练层数和任务表现的关系，第二个是训练参数数量和模型性能的关系。从中可以得到结论：第一，预训练模型中的每一层都包含用于解决目标任务的有用功能，说明多层有更多能力；第二，随着参数的增加，zero-shot获得更好的性能。简单总结来看就是，模型打了不仅能学到更多的知识，有助于解决下游任务，还表现出了zero-shot的能力
- ![GPT two plots](https://github.com/tinghe14/NLP-Papers/blob/3ca4f4dbb1d91e979d18a0c51aafacfa7c6fb9df/large%20language%20model/0%20gpt1%E7%9A%84%E4%B8%A4%E7%82%B9.png)
- Zero-Shot是指直接给模型任务输入让它输出任务结果；Few-Shot是给模型提供一些示例，然后再给出任务，让它给出输出结果。
- GPT2: 好了，有了上面的结论，很自然会怎么样？是不是想看看更多层（更多参数）的表现如何？于是半年多后GPT-2来了，参数量从GPT的110M增加到了1.5B，十倍于前者。更有意思的是，在GPT论文的[博客](https://openai.com/research/language-unsupervised)中有一个「未来工作」，放在第一个的就是扩大规模，还有两个分别是提升微调和更好地理解为什么生成式预训练能提升理解（NLU）能力。GPT发表于2018年6月，GPT-2发表于2019年2月，就是前者的升级版：一个是扩大规模，再就是Zero-Shot。如果说前者是观察到了这个现象，那后者就是进一步研究这个现象。
- GPT2中token生成的策略vs beam search: beam search两个明显问题：第一是生成的内容容易重复，第二十高质量的文本和高概率不一定相关。人更加喜欢有「不一样」的内容，而不是完全可预测的，比如张爱玲说过「孤独的人有他们自己的泥沼」，这种独一无二的文字用高概率的词大概率是得不到的。简单来看，这两个问题其实可以归结为一个点：生成的内容确定性太大。现在，介绍一种基于采样的方法：简单点来说，就是基于已有的上下文随机选择下一个Token。不过随机也有问题，那就是可能生成不连贯的文本（很容易理解对吧）。这里有个Trick可以缓解这个问题——进一步增加高概率词的可能性，降低低概率词的可能性。这样就不太容易随机到很低概率（很可能不连贯）的生成。具体的做法是用过一个temperature的参数调整输出的概率分布，这个参数值越大，分布就看起来越平滑，也就是高概率和低概率的差距拉小了（对输出不那么确定）；当然越小的话，高概率和低概率的差距更明显了（对输出比较确定）。如果趋近于0，那就和Greedy Search一样了。这在深度学习中是一种比较常见的方法，感兴趣的读者可以进一步读一下这个StackOverflow的[解释](https://stackoverflow.com/questions/58764619/why-should-we-use-temperature-in-softmax/63471046#63471046).除了这个trick, [这篇论文](https://arxiv.org/pdf/1805.04833.pdf)在2018年介绍了一种新的采样方案，简单但很有效果，就是gpt-2里面使用的top k 采样。简单来说，就是在选择下一个Token时，在Top-K个里面选（Top-K=0时就是所有词表范围）。这个方法不错，不过还有个小问题，就是Top-K个其实是一种硬截断，根本不管第K个概率是高还是低。极端情况下，如果某个词的概率到了0.99，K稍微大一点就必然会囊括进来一些很低概率的词。这会导致不连贯。于是，[2020年的这篇文章](https://arxiv.org/pdf/1904.09751.pdf)提出了另一个采样方案: top-p, GPT2里也有这个策略。这种策略是在累计概率超过p的词进行选择。这样，对于概率分布比较均匀的情况，可选的词就会多一些（可能要几十个词的概率和才会超过P）；对于概率分布不均匀的情况，可选的词就少一些（可能2-3个词的概率就超过了P）。Top-P看起来更加优雅一些，两者也可以结合使用，不过大部分时候当我们需要调的时候，调一个就好，包括前面的tempreture参数。如果要调多个，请确保理解每个参数的作用。最后，需要说明的是，任何一种采样策略都不能100%保证每一次生成的效果都很好，也没办法避免生成重复的话（可以考虑使用类似后处理的方法缓解）。也没有一种策略是在任何场景下都适用的，需要根据实际情况灵活选择。
- GPT-3: GPT-3是2020年7月发表的，在当时也是个大新闻，因为它的参数量已经达到了其他任何模型在当时望尘莫及的量级：175B，是GPT-2的100多倍。而且，没有开源。伸手党有点尴尬。
- 不微调 zero-shot: GPT-3是觉得既然有Zero-Shot能力，那能不能不微调呢，碰到一个任务就微调这多麻烦。你看看人类，只要几个例子（Few-Shot）和一些简单的说明就可以处理任务了是不是。怎么办？GPT-2不是进一步确认了Zero-Shot能力了吗，上，继续上，加大参数量，于是就有了175B的GPT-3。也就是说，各种任务来吧，我不调参数，顶多就要几个例子（下一步连这个也不要了），我就能给你搞定它。其实现在回头看，这篇论文是具有里程碑意义的，因为它从根本上触动了原有的范式，而且是革命式地触动。
- ![X shot在不同参数量级的表现 来自gpt-3 论文](https://github.com/tinghe14/NLP-Papers/blob/237fe905b66d04b9953eba049b482227cdf3c6ed/large%20language%20model/0%20X%20shot%E5%9C%A8%E4%B8%8D%E5%90%8C%E5%8F%82%E6%95%B0%E9%87%8F%E7%BA%A7%E7%9A%84%E8%A1%A8%E7%8E%B0%20%E6%9D%A5%E8%87%AAgpt-3%20%E8%AE%BA%E6%96%87.png)
- 图：提供几个信息：x-shot在不同量级差别巨大，大模型就是有超能力；大模型下，one-shot效果明显大幅度提升；增加prompt会进一步大幅度提升；few-shot的边际收益在递减，大概8-shot一下，prompt作用明显，但从One-Shot到8-Shot，Prompt的作用也在递减。超过10-Shot时，Prompt基本没作用了。
-  改变范式：总而言之，大模型具有In-Context能力，这种能力使得它不需要针对不同任务再进行适应性训练（微调），它用的就是它自己本身的理解力。这本来应该很让人震惊（甚至有一丢丢惊恐），不过大家可能都先被它的价格和规模震惊到了。接下来，我们在直观感受一下利用这种In-Context能力完成任务的方式，如下图所示：
-  ![in-context能力完成任务 来自gpt 3]()
-  



