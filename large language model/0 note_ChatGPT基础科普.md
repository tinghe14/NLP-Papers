# LM
LM, language model, 语言模型
- 定义：利用自然语言文本构建的，根据输入的文字，输出相应文字的模型。也叫做decoder解码策略
- token:  如果只是用26个英文字母，虽然词表很小（加上各种符号可能就100来个），但粒度太细，每个Token几乎完全没法表示语义；如果用词，这个粒度又有点太大，尤其英文还有不同时态，其实它们意思差不多，只是后面不一样。所以子词就应运而生——它把一个词拆成一定大小的语义单位，每个单位既可以表示一定含义，又能灵活组合。
- greedy search vs beam search: 因为我们考虑的是最大概率，每次都会输出同样的话即为greedy search; 语言模型都会在这个地方做点策略，让模型多看几个可能的单词。而不是就看最高的那一个，这样继续往下找的时候，你会发现到下一步，刚刚最大概率的词，如果加上这一步的词，他的路径可能没有刚刚小一点概率的词的路径大.beam search: 简单的来说就是一步多看几个词，看最终句子（比如生成到句号，感叹号或者其他停止符号）的概率。看得越多，约不容易生成固定的文本

### Ngram
- 最早的LM Ngram：把一句话切成一个个的词，然后统计概率，比如Bi-gram:下一个词是根据前两个词来的
- Ngram的问题 离散：在计算机中，只能用1和0表示某个词。假设词表大小为50000，刚刚Bi-Gram中，「我喜欢」这个Gram就是49999个0和1个1组成的稀疏向量。

### Embedding
- embedding的好处：一种稠密表示方法，简单来说，一个Token是很多个小数（这个很多可以是任意多个，专业称呼为Embedding的维度，看用的模型和设定的参数），一般数字越多，模型越大，表示能力越强。
- 用embedding怎么预测下一个token: 还是算概率，但这次和刚刚有点不一样了，刚刚离散的那就是统计出来有多少次除以总词数就是出现概率。但是稠密向量要稍微换个方式，也就是说，给你一个d维的向量，你最后要输出一个长度为N的向量，N是词表大小，N中每一个值都是一个概率值，表示Token的概率，加起来为1。W即为模型的参数，其实X也可以看作是参数自动学习到。因为我们知道了输入和输出的大小，所以中间其实可以经过任意随意的计算，总之就是各种张量（三维以上数组）运算，只要保证最后的形式不变就行。这中间的各种计算就意味着各种不同的模型。
~~~
X = [0.001, 0.002, 0.0052, ..., 0.0341] # d维，加起来和1没关系，大小是1×d
Y = [0.1, 0.5, ..., 0.005, 0.3] # N个，加起来=1，大小是1×N
W·X = Y  # W自然可以是 d×N 维的矩阵
~~~

### RNN
- 深度学习初期 最著名的语言模型Recurrent Neural Network RNN: RNN模型与其他神经网络不同的地方在于，它的节点之间存在循环链接，这使得它能够记住之前的信息，并将它们应用于当前的输入。这种记忆能力使得 RNN 在处理时间序列数据时特别有用，例如预测未来的时间序列数据、自然语言处理等 

![0 RNN模型参数介绍](https://github.com/tinghe14/NLP-Papers/blob/main/large%20language%20model/0%20RNN%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E4%BB%8B%E7%BB%8D.png)
